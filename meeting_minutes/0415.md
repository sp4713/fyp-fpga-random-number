Next meeting: 22 Apr 2:00pm
#Discussion Points
- use histogram at the end of the design
- Kernel can input seed, output current seed for reusability
- assume software is all-knowing about histogram properties (size, what data represents, etc)
- think about resolving pipelining issue of accumulators (since HLS implementation probably consumes 3 clock cycles for 3 clocked register)
	- consider ping pong buffers (there might be better ways than this)

##Introduction
- long, deep streams
- highlight consumption of numbers being really fast, hence testing needs to be really fast - in Why Test Random Number Sequences
- mention at this stage how later in interim report RNG and their fallacies will be discussed

####Chi-squared test
The chi-squared test used in this project is essentially producing a sequence of random numbers and not using hypothesis testing
- not traditional use of hypothesis testing
- assumption of high number of buckets in design is met

##Project Specifications
- explain what the deliverables are (import Evaluation in a more high-level form)

####Empirical Tests under consideration
- mention how it's probably a good idea to speed up shared components, in this case probably histogram is a main component of all tests
- identifying opportunities of sharing parts of testing methodology -> easier to be done in hardware rather than in software
- for theoretical testings, mention how there is knowledge out there already on how these RNGs fail, aim is to provide empirical testing
- for theoretical tests, they mainly consider over the entire sequence, not subsequences like the empirical tests do
####Implementation plan
- try align with Gantt chart, e.g. when risks should be assessed and when I am going to decide what to do
- think of one big HLS core, think of how CPU gives kernel parameters, how does it tell to start/finish, how does it get results back
	- if kernel looks like 
- mention use of testbenches after implementation of each test prior to deployment on the FPGA
- try turn functions into stateless function for robust design
  - perhaps checkpointable -> mention this in implementation
##Planning and Evaluation
- if people were to read this section, would other people be able to determine performance or not? make it very clear
- explain how design will be evaluated
- be clear on how performance is measured (e.g. throughput is numbers or time?)
- break down throughput
	- basically like CPU design (clock rate, cycles per instruction), actually 	care about instructions/second
- sketch the means by which user will receive
	- mock out the output
	- if say csv, show the format based on existing tests (what statistics are required in association with the p-values)ta 
		- how far into the testing was this snapshot taken 		(assuming will use interruptable or feedback during load)
	- explain how will deal with power interruptions, other hazards during running
- consider checkpoints (making design intrinsically robust rather than recoverable) flushing out (consider implementing this purely in software, maybe with an additional input required by the user) dogma: hardware as simple/reusable as possible, move complexity to software
- Gantt chart include past and future with report submission inbetween as a milestone
